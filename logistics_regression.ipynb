{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Logistics Regression using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import all the neccessary libraries\n",
    "import pandas as pd # data processing\n",
    "import os #helps changing directory and locating your file\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() #to make sure all variables are on the same scale. It improves the computation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing libraries for modeling\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing directory to folder where file has kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\blog\\\\titanic'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "os.chdir('D:\\\\blog/titanic')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the famous titanic dataset. Here we trying to predict who survives or dies based on certain features.\n",
    "\n",
    "Please note: I have already performed data preprocessing and one hot encoding on my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df= pd.read_csv('titanic_df_cleaned.csv')\n",
    "#titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating target column and dropping from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=titanic_df['Survived'].iloc[:891]#target variable\n",
    "y_train = y_train.astype(int)\n",
    "x_train=titanic_df.drop('Survived', axis=1)#dropping target variable\n",
    "#y_train.tail()\n",
    "#x_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using standard scaler to transform data on same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = scaler.fit_transform(x_train)\n",
    "x_scaled = pd.DataFrame(x_scaled, columns = x_train.columns)\n",
    "#x_scaled.tail()#All variables are now in same scale of 0 to 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 59)\n",
      "(418, 59)\n"
     ]
    }
   ],
   "source": [
    "train_scaled= x_scaled.iloc[:891] #train dataset\n",
    "print(train_scaled.shape)\n",
    "test_scaled=x_scaled.iloc[891:]#final dataset\n",
    "print(test_scaled.shape)\n",
    "#Creating training and test dataset from from training dataset\n",
    "train_x,test_x,train_y,test_y = train_test_split(train_scaled,y_train, random_state = 101, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a basic logistics regression model, fitting on train set and checking accuracy on both train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "0.8577844311377245\n",
      "0.7802690582959642\n",
      "Wall time: 75.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr=LogisticRegression(random_state =0)\n",
    "lr.fit(train_x,train_y)\n",
    "print(lr)\n",
    "print(lr.score(train_x,train_y))\n",
    "print(lr.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hyper tune our logistics regression model using Gridsearch. Gridsearch will insert hyper parameters in lr_param_grid and check which combination provides the best result. We are storing the best result in lr_best and checking accuracy on the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=300,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=0, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "{'C': 0.1, 'dual': False, 'max_iter': 300, 'penalty': 'l2', 'solver': 'saga', 'tol': 0.0001}\n",
      "0.8322026232473994\n",
      "0.7982062780269058\n",
      "Wall time: 9.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Meta modeling with Logistics Regression\n",
    "lr_param_grid = {'penalty' : ['l1', 'l2'], \n",
    "               'C' : [0.1, 1, 0.015],\n",
    "              'max_iter' : [100,300, 400],\n",
    "              'dual':[False, True],\n",
    "            'tol': [0.0001,0.001],\n",
    "            'solver': ['lbfgs', 'sag', 'saga']\n",
    "             },\n",
    "\n",
    "gs_lr = GridSearchCV(lr,lr_param_grid, cv=10, scoring=\"accuracy\",n_jobs=6)\n",
    "gs_lr.fit(train_x,train_y) \n",
    "lr_best = gs_lr.best_estimator_\n",
    "print(lr_best)\n",
    "print(gs_lr.best_params_)\n",
    "print(gs_lr.best_score_)\n",
    "#let's validate our prediction\n",
    "\n",
    "print(lr_best.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will try to predict the unseen data using our best model and check the output \n",
    "lr_prediction=lr_best.predict(test_scaled)\n",
    "lr_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
